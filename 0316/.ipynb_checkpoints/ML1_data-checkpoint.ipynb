{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1) Create dataset (make_blobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-5ead53ed7c1f>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-5ead53ed7c1f>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    from sklearn.preprocessing as preprocessing\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = [[-5, 0], [0, 1.5], [5, -1]]\n",
    "# make_blobs(n_samples, centers[, random_state])\n",
    "x,y = make_blobs(n_samples=1000,centers=centers, random_state=40)\n",
    "color='rgb'\n",
    "color= [color[y[i]] for i in range(len(y))]\n",
    "plt.scatter(x[:,0],x[:,1],c=color)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2) Training and Testing data Split  \n",
    "把資料集切分成training data跟testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#製造X=維度10x5,值為0-49的矩陣\n",
    "#製造y=0-9的一維矩陣\n",
    "X, y = np.arange(50).reshape(10,5), np.arange(10)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state= 42, shuffle=True)\n",
    "\n",
    "print(x_train)\n",
    "print(x_test)\n",
    "print(y_train)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3) data normalization\n",
    "資料各維度因為尺度, 單位不同, 導致影響, 需要做normalization\n",
    "- Z-Score scale  \n",
    "轉成標準常態分佈,平均值為0, 標準差為1\n",
    "- Maxmin Scale  \n",
    "將最大設為1, 最小設為0\n",
    "\n",
    "```python\n",
    "from sklearn import preprocessing\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#z-score\n",
    "# a是3筆4維資料\n",
    "a = np.array([[10, 2.7, 3.6, 5],\n",
    "              [-100, 5, -2, 10],\n",
    "              [120, 20, 40, 50]], dtype=np.float64)\n",
    "\n",
    "scale_a = preprocessing.scale(a, axis=0)\n",
    "print(scale_a.std(axis=0))\n",
    "print(scale_a.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_a = preprocessing.minmax_scale(a, axis=0, feature_range=(0,1)) #default feature range 0~1\n",
    "print(scale_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "### 標準化範例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_classification產生n_samples=300筆資料, n_classes=2表示y輸出2個類別\n",
    "# 每筆資料n_features=2維度為2, n_informative=2有用的維度有2個, n_redundant=0表示redundant維度0個\n",
    "# n_clusters_per_class表示每個class中的clusters數量\n",
    "# shift=0,scale=100表示將資料先偏移shift再乘上scale倍數放大\n",
    "from sklearn.datasets.samples_generator import make_classification \n",
    "# 製作兩個feature的分類資料\n",
    "X, y = make_classification(\n",
    "    n_samples=300, n_classes=2, n_features=2,\n",
    "    n_redundant=0, n_informative=2, \n",
    "    random_state=22, n_clusters_per_class=1, \n",
    "    shift=0,scale=100)\n",
    "print(X.shape)\n",
    "# 將資料可視化\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import svm classifier\n",
    "from sklearn.svm import SVC\n",
    "X, y = make_classification(\n",
    "    n_samples=300, n_classes=2, n_features=2,\n",
    "    n_redundant=0, n_informative=2, \n",
    "    random_state=22, n_clusters_per_class=1, \n",
    "    shift=0,scale=100)\n",
    "#將資料分成訓練及測試集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)\n",
    "model = SVC(gamma='auto')\n",
    "# 訓練model\n",
    "model.fit(X_train, y_train)\n",
    "# 用model預測整個資料集X\n",
    "prediction = model.predict(X)\n",
    "# model.score(測試集, 測試集label)可以回傳accuracy\n",
    "print('accuracy before normalization:%.2f'%model.score(X_test, y_test))\n",
    "# 畫出原始資料\n",
    "plt.subplot(121)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.title('Actual')\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "# 畫出預測結果\n",
    "plt.subplot(122)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=prediction)\n",
    "plt.title('Prediction')\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#將資料做z-score normalization\n",
    "X, y = make_classification(\n",
    "    n_samples=300, n_classes=2, n_features=2,\n",
    "    n_redundant=0, n_informative=2, \n",
    "    random_state=22, n_clusters_per_class=1, \n",
    "    shift=0,scale=100)\n",
    "X = preprocessing.scale(X, axis=0)\n",
    "#將資料分成訓練及測試集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)\n",
    "# import svm classifier\n",
    "from sklearn.svm import SVC\n",
    "model = SVC(gamma='auto')\n",
    "model.fit(X_train, y_train)\n",
    "prediction = model.predict(X)\n",
    "print('accuracy after normalization:%.2f'%model.score(X_test, y_test))\n",
    "# 畫出原始資料\n",
    "plt.subplot(121)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.title('Actual')\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "# 畫出預測結果\n",
    "plt.subplot(122)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=prediction)\n",
    "plt.title('Prediction')\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (4.1) Label Encoder\n",
    "sklearn.preprocessing的LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "encX = LabelEncoder()\n",
    "encX.fit(['看電視','讀書','音樂','游泳'])\n",
    "ency = LabelEncoder()\n",
    "ency.fit(['是', '否'])\n",
    "#製作df\n",
    "data_Xy = {'興趣':['看電視','讀書','音樂','看電視'],'成功與否':['是','否','否','是']}\n",
    "df = pd.DataFrame(data = data_Xy, index=['小明','小林','小英','小陳'])\n",
    "df = df[['興趣','成功與否']]\n",
    "print(df)\n",
    "#\n",
    "print(encX.transform(df['興趣']))\n",
    "\n",
    "df_encode = df.copy()\n",
    "df_encode['興趣'] = encX.transform(df_encode['興趣'])\n",
    "df_encode['成功與否'] = ency.transform(df_encode['成功與否'])\n",
    "\n",
    "df_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.array([1,0,0,1])\n",
    "#將encoder反解\n",
    "df['prediction'] = ency.inverse_transform(prediction) \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (4.2) Label Encoder (Map)\n",
    "有時候需要手動指定類別的值, 例如S/M/L/XL有強弱之分, 直接用MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#\n",
    "size_mapping = {'S':0, 'M':1, 'L':2, 'XL':3} #define map as dictionary\n",
    "label_mapping = {'否':0, '是':1} #define map as dictionary\n",
    "\n",
    "data_Xy = {'衣服size':['XL','S','M','L'],'成功與否':['是','否','否','是']}\n",
    "df = pd.DataFrame(data = data_Xy, index=['小明','小林','小英','小陳'])\n",
    "df = df[['衣服size','成功與否']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encode = df.copy()\n",
    "df_encode['衣服size'] = df_encode['衣服size'].map(size_mapping)\n",
    "df_encode['成功與否'] = df_encode['成功與否'].map(label_mapping)\n",
    "df_encode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (4.3) Label Encoder (Map)\n",
    "類別的值沒有大小之分時, 用one-hot encoding  \n",
    "原本有類別標籤有看電視, 讀書, 音樂3種  \n",
    "分成 [是否看電視, 是否讀書, 是否音樂] 3個維度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data_Xy = {'興趣':['看電視','讀書','音樂','看電視'],'成功與否':['是','否','否','是']}\n",
    "df = pd.DataFrame(data = data_Xy, index=['小明','小林','小英','小陳'])\n",
    "df = df[['興趣','成功與否']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.iloc[:,0]表示第0維[興趣], 有看電視, 讀書, 音樂3種, 所以會分成3個維度\n",
    "X = pd.get_dummies(df.iloc[:,0]) #針對第一個欄位做get_dummies\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (5) Model Evaluation\n",
    "- MAE: Mean Absolute Error  \n",
    "缺點: 沒有標準化, 不好判定值是大是小  \n",
    "缺點: 離群值會影響MAE(因為離群值的|誤差|會大於正常值)  \n",
    "- MSE: Mean Square Error  \n",
    "缺點: 沒有標準化, 不好判定值是大是小  \n",
    "缺點: 離群值會影響MAE(因為離群值的$(誤差)^2$會遠遠大於正常值)  \n",
    "- $R^2$ Score: Coefficient of determination  \n",
    "$R^2(y,\\hat{y}) = 1 - \\frac{\\sum(y_i-\\hat{y_i})^2}{\\sum{(y_i - \\overline y)^2}}$\n",
    "- F1-score: \n",
    "$2\\frac{1}{\\frac{1}{recall}+\\frac{1}{precision}}$  \n",
    "recall= $\\frac{TP}{FN+TP}$  \n",
    "precision= $\\frac{TP}{TP+FP}$  \n",
    "\n",
    "| * | Predict Neg | Predict Pos |\n",
    "|---|----- | ----- |\n",
    "|True Neg| TN | FP |\n",
    "|True Pos | FN | TP |\n",
    "\n",
    "\n",
    "- Accuracy:  \n",
    "$\\frac{TF+TP}{TN+FN+TP+FP}$\n",
    "\n",
    "```python\n",
    "from sklearn import metrics\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 8, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 產生 n_samples筆資料, 資料X的維度n_features, 輸出y的維度有n_targets, noise越大資料越散開\n",
    "X, y = datasets.make_regression(n_samples=100,n_features=1,n_targets=1, random_state = 42, noise = 20)\n",
    "plt.scatter(X,y)\n",
    "model = LinearRegression()\n",
    "# Train model\n",
    "model.fit(X, y)\n",
    "# Predict X\n",
    "prediction = model.predict(X)\n",
    "# draw\n",
    "plt.scatter(X,prediction)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_prediction(plot_dict):\n",
    "    for noise in plot_dict:\n",
    "        X, y = datasets.make_regression(n_features=1, random_state = 42, noise = noise)\n",
    "        model = LinearRegression()\n",
    "        model.fit(X, y)\n",
    "        prediction = model.predict(X)\n",
    "        # y is true label\n",
    "        mae = metrics.mean_absolute_error(prediction, y)\n",
    "        mse = metrics.mean_squared_error(prediction, y)\n",
    "        r2 = metrics.r2_score(prediction, y)\n",
    "        plt.subplot(plot_dict[noise])\n",
    "        plt.xlabel('prediction')\n",
    "        plt.ylabel('actual')\n",
    "        plt.tight_layout()\n",
    "        plt.plot(prediction, y,'.')\n",
    "        plt.title('Plot for noise: %d'%noise + '\\n' + 'mae:%.2f'%mae\n",
    "                 + '\\n' + 'mse:%.2f'%mse\n",
    "                 + '\\n' + 'r2:%.2f'%r2)\n",
    "    plt.show()\n",
    "# 141 use for subplot 1x4 and index 1\n",
    "# 142 use for subplot 1x4 and index 2\n",
    "plot_dict = {1:141, 9:142, 18:143, 1000:144}\n",
    "linear_prediction(plot_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手寫辨識"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "\n",
    "digits = load_digits()\n",
    "# split the data into training and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.25, random_state=0)\n",
    "\n",
    "actual = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume we already got predicted\n",
    "predicted = np.array([2, 8, 2, 6, 6, 7, 1, 9, 8, 5, 2, 8, 6, 6, 6, 6, 1, 0, 5, 8, 8, 7, 8,\n",
    "       4, 7, 5, 4, 9, 2, 9, 4, 7, 6, 8, 9, 4, 3, 1, 0, 1, 8, 6, 7, 7, 1, 0,\n",
    "       7, 6, 2, 1, 9, 6, 7, 9, 0, 0, 9, 1, 6, 3, 0, 2, 3, 4, 1, 9, 2, 6, 9,\n",
    "       1, 8, 3, 5, 1, 2, 8, 2, 2, 9, 7, 2, 3, 6, 0, 5, 3, 7, 5, 1, 2, 8, 9,\n",
    "       3, 1, 4, 7, 4, 8, 5, 8, 5, 5, 2, 5, 9, 0, 7, 1, 4, 7, 3, 4, 8, 9, 7,\n",
    "       9, 8, 2, 1, 5, 2, 5, 8, 4, 1, 7, 0, 6, 1, 5, 5, 9, 9, 5, 9, 9, 5, 7,\n",
    "       5, 6, 2, 8, 6, 9, 6, 1, 5, 1, 5, 9, 9, 1, 5, 3, 6, 1, 8, 9, 8, 7, 6,\n",
    "       7, 6, 5, 6, 0, 8, 8, 9, 8, 6, 1, 0, 4, 1, 6, 3, 8, 6, 7, 4, 9, 6, 3,\n",
    "       0, 3, 3, 3, 0, 7, 7, 5, 7, 8, 0, 7, 1, 9, 6, 4, 5, 0, 1, 4, 6, 4, 3,\n",
    "       3, 0, 9, 5, 9, 2, 1, 4, 2, 1, 6, 8, 9, 2, 4, 9, 3, 7, 6, 2, 3, 3, 1,\n",
    "       6, 9, 3, 6, 3, 2, 2, 0, 7, 6, 1, 1, 9, 7, 2, 7, 8, 5, 5, 7, 5, 3, 3,\n",
    "       7, 2, 7, 5, 5, 7, 0, 9, 1, 6, 5, 9, 7, 4, 3, 8, 0, 3, 6, 4, 6, 3, 2,\n",
    "       6, 8, 8, 8, 4, 6, 7, 5, 2, 4, 5, 3, 2, 4, 6, 9, 4, 5, 4, 3, 4, 6, 2,\n",
    "       9, 0, 6, 7, 2, 0, 9, 6, 0, 4, 2, 0, 7, 9, 8, 5, 7, 8, 2, 8, 4, 3, 7,\n",
    "       2, 6, 9, 9, 5, 1, 0, 8, 2, 8, 9, 5, 6, 2, 2, 7, 2, 1, 5, 1, 6, 4, 5,\n",
    "       0, 9, 4, 1, 1, 7, 0, 8, 9, 0, 5, 4, 3, 8, 8, 6, 5, 3, 4, 4, 4, 8, 8,\n",
    "       7, 0, 9, 6, 3, 5, 2, 3, 0, 8, 8, 3, 1, 3, 3, 0, 0, 4, 6, 0, 7, 7, 6,\n",
    "       2, 0, 4, 4, 2, 3, 7, 1, 9, 8, 6, 8, 5, 6, 2, 2, 3, 1, 7, 7, 8, 0, 3,\n",
    "       3, 2, 1, 5, 5, 9, 1, 3, 7, 0, 0, 3, 0, 4, 5, 8, 3, 3, 4, 3, 1, 8, 9,\n",
    "       8, 3, 6, 3, 1, 6, 2, 1, 7, 5, 5, 1, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 6))  # figure size in inches\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "# plot the digits: each image is 8x8 pixels\n",
    "for i in range(64):\n",
    "    # subplot 8x8\n",
    "    ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow( X_test.reshape(-1, 8, 8)[i], cmap=plt.cm.binary,\n",
    "              interpolation='nearest')\n",
    "    \n",
    "    # label the image with the target value\n",
    "    if predicted[i] == actual[i]:\n",
    "        ax.text(0, 7, str(predicted[i]), color='green')\n",
    "    else:\n",
    "        ax.text(0, 7, str(predicted[i]), color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantitative Measurement on the Performance\n",
    "在分類問題上，所有的模型評估方法基本上都可以由confusion matrix得出來。  \n",
    "在y方向也就是row方向代表的就是actual 0~9，  \n",
    "x方向代表也就是column方向代表的是predicted 0~9。 \n",
    "```\n",
    "actual/predicted       0  1  2  3  4  5  6  7  8   9\n",
    "         0          [[37  0  0  0  0  0  0  0  0  0]\n",
    "         1           [ 0 40  0  0  0  0  1  0  1  1]\n",
    "         2           [ 0  0 42  2  0  0  0  0  0  0]\n",
    "         3           [ 0  0  0 44  0  0  0  0  1  0]\n",
    "         4           [ 0  0  0  0 37  0  0  1  0  0]\n",
    "         5           [ 0  0  0  0  0 46  0  0  0  2]\n",
    "         6           [ 0  1  0  0  0  0 51  0  0  0]\n",
    "         7           [ 0  0  0  1  1  0  0 46  0  0]\n",
    "         8           [ 0  3  1  0  0  0  0  0 44  0]\n",
    "         9           [ 0  0  0  0  0  1  0  0  2 44]]\n",
    "         ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "# 可以用metrics.confusion_matrix列出\n",
    "print(metrics.confusion_matrix(actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics.accuracy_score計算accuracy\n",
    "print('accuracy:%.3f'%(metrics.accuracy_score(actual, predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision Recall & F1-score\n",
    "使用metrics.classification_report  \n",
    "support 43表示有43個1分對了0.91也就是約40個分對"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(actual, predicted)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myKernel",
   "language": "python",
   "name": "mykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
