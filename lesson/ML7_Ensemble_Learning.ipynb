{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning / Ensemble Method\n",
    "\n",
    "將各自獨立(independent)且歧異性(diverse)夠高的數個弱分類器(more accurate than random guessing)集成  \n",
    "- 弱分類器有各自的偏見跟觀點, 集成可以相互消除各自的偏見, 集合大家的觀點  \n",
    "- 假設有3個accuracy 0.6的弱分類器, 採多數決(voting), 那麼就是3個分類器都說是true或其中2個說是true  \n",
    "$C(3,3)*0.6*0.6*0.6 + C(3,2)*0.6*0.6*0.4 = 0.648$  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enseble Learning\n",
    "- Bagging  - resample training data  \n",
    "Random Forest\n",
    "- Boosting - reweight trainiing data + weight method  \n",
    "Adaboost  \n",
    "Gradient Boosting(XGBoost)  \n",
    "- Stacking  - blending weak learners  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging (Bootstrap aggregating)  \n",
    "- Bootstrap:  \n",
    "Draw n' out of n data instances (n' < n), usually with replacement  \n",
    "- Bootstrap aggregating:  \n",
    " 1. Repeat Boostrap for m times  \n",
    " 1. Train a model for each sample dataset  \n",
    " 1. combine the models to make prediction  \n",
    "- Random Forest:  \n",
    " bagging + randomized feature set  \n",
    " 1. build many decision tree classifiers (or regressors)   \n",
    " each tree is trained on a subset of the training data (bagging)  \n",
    " each tree use a subset of the features\n",
    " 1. combine the prediction of each tree (e.g., average or majority voting) \n",
    " \n",
    "  決策樹分類的圖像化可以看到明顯的矩型, 隨機森林樹越多在邊角會越呈現圓滑狀  \n",
    "  隨機森林因為往往有上百棵樹, 所以predict需要一定的計算量  \n",
    "  優點是可以一開始就隨機做好所有resample dataset跟randomized feature set 平行訓練所有的樹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "- Assign different weight to different samples\n",
    "  對於每筆訓練資料會有各自的weight\n",
    "- \"weighted\" combination of models  \n",
    "  訓練出來的弱分類器, 會依據分類能力的好壞, 在predict時的影響力有所不同  \n",
    "- 建立新的弱分類器時會嘗試將常分錯, 不好分的sample盡量分對  \n",
    "- **Adaboost (Adaptive Boosting):**  \n",
    "$\\begin{aligned}\n",
    "Training:&\\\\ \n",
    "& Set~uniform~weights~to~each~instance~~~i.e.,~~w_i^{(0)} = \\frac{1}{n}\\\\\n",
    "& for~k~=~1~to~k:& \\\\\n",
    "& ~~~~Train~~f_k~~by~minimizing~~(weighted)~~error \\\\\n",
    "& ~~~~compute~Weighted~Error~of~training~instance~using~f_k  \\\\\n",
    "& ~~~~set~\\alpha_k,~the~weight~of~f_k~based~on~Weighted~Error  \\\\\n",
    "& ~~~~set~w_i^{(k)},~the~weight~of~each~instance~based~on~Ensemble~Prediction \\\\\n",
    "\\end{aligned}$  \n",
    "\n",
    " k是hyper parameter決定迭代次數  \n",
    " $f_k$是每次訓練得到的弱分類器  \n",
    " $\\alpha_k = 0.5 * log(\\frac{1-err^{(k)}}{err^{(k)}})$,  \n",
    " > $\\alpha_k \\in [-\\infty, \\infty]$  \n",
    " > $log(1)=0$, 表示$err^{(k)}=0.5$跟亂猜一樣  \n",
    " > $err^{(k)}=0, \\alpha_k=\\infty$表示可信度高  \n",
    " > $err^{(k)}=1, \\alpha_k=-\\infty$表示盡量往反方向猜  \n",
    " \n",
    " $err^{(k)} = \\sum_iW_i^{(k-1)}*\\xi_k(f_k(x_i),y_i)$,  \n",
    " > 該資料$x_i$用這次$f_k$分錯誤差再乘以上次算出該資料$x_i$權重, 再將所有資料$x_i$加總起來  \n",
    "\n",
    " $W_i^{(k)} = \\frac{W_i^{(k-1)}~e^{(-\\alpha_ky_i\\hat y_i)}}{z^{(k)}}$  \n",
    " > $z^{(k)}$是$\\sum_iW_i^{(k)}$ 用作normalization term  \n",
    " > $exp(-\\alpha_ky_i\\hat y_i)$, 如果分對$y_i\\hat y_i$會是正值,指數函數使值會在0~1之間, 使$W_i$變小, $\\alpha_k$指這次的分類器分類能力, 越大表示可信度越大  \n",
    " 如果分錯$y_i\\hat y_i$會是負值,指數函數使值會在1~$\\infty$之間,  $\\alpha_k$越大值越接近$\\infty$,權重越大  \n",
    " \n",
    " $Prediction:$  \n",
    " - $\\hat y_{test} = \\alpha_1f_1(x_{test}) + \\alpha_2f_2(x_{test})...+\\alpha_kf_k(x_{test})$  \n",
    "   Predicted classes = $sign(\\hat y_{test})$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Gradient Boosting for regression:**  \n",
    " 1. Given D = {($x_1$,$y_1$),($x_2$,$y_2$),...,($x_n$,$y_n$)}  \n",
    " 1. Train a model $f_1$ to fit D, and let F = $f_1$  \n",
    " 1. Train a model $f_2$ to fit the residuals given the features  \n",
    " i.e. fitting {($x_1$,$y_1-F(x_1)$),($x_2$,$y_2-F(x_2)$),...,($x_n$,$y_n-F(x_n)$)}  \n",
    " Let F = $f_1+f_2$  \n",
    " 1. Repeat the process to get $f_3,f_4,....$  \n",
    " F = $f_1(x_i)+f_2(x_i)+f_3(x_i)+...f_n(x_i)$  \n",
    " \n",
    " 可以想作要找到一組參數使得F(x)跟實際y越接近越好  \n",
    " 令loss function J = $\\frac{1}{2}\\sum_i(y_i - F(x_i))^2$  \n",
    " 訓練出第一個model $f_1$, 令$F(x_j) = f_1(x_j)$  \n",
    " 用Gradient descent持續迭代更新F(X)直到收斂  \n",
    " $F^{(k+1)}(x_j) = F^{(k)}(x_j) - \\frac{\\partial J}{\\partial F(x_j)} $  \n",
    " 因為$\\frac{\\partial J}{\\partial F(x_j)}= -(y_j-F(x_j))$  \n",
    " 所以$F^{(k+1)}(x_j) = F^{(k)}(x_j)+(y_j-F^{k}(x_j))$  \n",
    " - 決策樹建立一個相對複雜的tree來預測問題, Gradient Boosting使用多個Simple tree來預測問題  \n",
    " - 隨機森林產生很多樹來預測問題, 這些樹相互獨立, Gradient Boosting使用多個Simple tree來預測問題, 新的樹試著去將之前的樹的誤差分對  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking  \n",
    "將各種不同的分類器輸出重新當作輸入  \n",
    "試著去重新訓練出一個ensemble model  \n",
    "ensemble model的output是label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 隨機森林\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "```  \n",
    "**[參數]**  \n",
    "- n_estimators: 生成樹的數量  \n",
    "- max_features: 隨機抽取feature的數量  \n",
    "> If “auto”, then max_features=sqrt(n_features).  \n",
    "> If “sqrt”, then max_features=sqrt(n_features) (same as “auto”).  \n",
    "> If “log2”, then max_features=log2(n_features).  \n",
    "> If None, then max_features=n_features.  \n",
    "\n",
    "- max_depth: 如果是None表示分到leaves都是pure或leaves數量小於min_samples_split為止  \n",
    "             其他數值決定樹的深度  \n",
    "- min_samples_leaf := 1   \n",
    "- min_samples_split := 2  \n",
    "\n",
    "Model可以看importance知道哪個feature最常被用來split:   \n",
    "```python\n",
    "model.feature_importances_\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.25, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=123456, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=100,criterion='gini', max_depth=None, max_features='auto', min_samples_leaf=1, min_samples_split=2, random_state=123456)\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9210526315789473"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = model.predict(X_test)\n",
    "metrics.accuracy_score(y_pred=predicted, y_true=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.07765363, 0.03215788, 0.4090637 , 0.48112478])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.feature_names)\n",
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n",
    "```python\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "```  \n",
    "**[參數]**  \n",
    "- n_estimators: 生成樹的數量  \n",
    "- loss : {‘deviance’, ‘exponential’}  \n",
    "loss function to be optimized  \n",
    "default使用deviance  \n",
    "- learning_rate:  (default=0.1)  \n",
    "shrinks the contribution of each tree by learning_rate  \n",
    "縮小每個model的權重, 這樣可以訓練出更多的樹  \n",
    "- criterion : (default=”friedman_mse”)  \n",
    "The function to measure the quality of a split.  \n",
    "Mean squared error with improvement score by Friedman  \n",
    "“mse” or “mae”  \n",
    "\n",
    "- max_features: 隨機抽取feature的數量  \n",
    "> If “auto”, then max_features=sqrt(n_features).  \n",
    "> If “sqrt”, then max_features=sqrt(n_features) (same as “auto”).  \n",
    "> If “log2”, then max_features=log2(n_features).  \n",
    "> If None, then max_features=n_features.  \n",
    "\n",
    "- max_depth: (default=3)\n",
    "           決定樹的深度  \n",
    "- min_samples_leaf := 1   \n",
    "- min_samples_split := 2  \n",
    "\n",
    "Model可以看importance知道哪個feature最常被用來split:   \n",
    "```python\n",
    "model.feature_importances_\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "data = load_iris()\n",
    "#X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.25, random_state=5)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target,test_size=0.25,random_state=5)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              n_iter_no_change=None, presort='auto', random_state=123456,\n",
       "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GradientBoostingClassifier(n_estimators=100,criterion='friedman_mse',  loss='deviance',learning_rate=0.1,max_depth=3, max_features=None, min_samples_leaf=1, min_samples_split=2,random_state=123456)\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9210526315789473"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = model.predict(X_test)\n",
    "metrics.accuracy_score(y_pred=predicted, y_true=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.02463981, 0.00749258, 0.29868364, 0.66918397])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.feature_names)\n",
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myKernel",
   "language": "python",
   "name": "mykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
