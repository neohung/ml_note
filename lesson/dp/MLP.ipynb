{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手刻神經網絡 with numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numpy.random.seed(number) → 控制output 結果  \n",
    "numpy.random.random((size=None)) → 建立一個數值為0到1之間的隨機array  \n",
    "numpy.zeros((shape)) → 建立一個全部為0的矩陣  \n",
    "numpy.matmul(matrix1, matrix2) → 矩陣相乘  \n",
    "numpy.vstack(matrix1, matrix2) → 已列的方式對array或matrix做組合  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Derivative 微分\n",
    "\n",
    "###  (1)先寫出一個簡單的方程式：  $ f(x) = 0.05x^2 + 0.8x $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_function(x):\n",
    "    y = 0.05*x**2 + 0.8*x\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2)定義微分公式：$ \\frac{df(x)}{dx} =\\lim\\limits_{h\\to 0} \\frac{(f(x+h) - f(x))}{h} $\n",
    "> 提示：不要將 $ epsilon $設的太大或是太小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define derivative, f是一個函數\n",
    "epsilon = 0.1\n",
    "def derivative(f, x, epsilon=0.1):\n",
    "    # f is our function, and x is our variable\n",
    "    h = epsilon\n",
    "    return (f(x+h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3)用我們的公式算 $x=5$ 和 $x=10$ 的微分\n",
    ">可以嘗試改變epsilon來觀察微分的變化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3049999999999962\n",
      "1.8050000000000033\n",
      "0.0\n",
      "0.0\n",
      "1.3000049999956076\n",
      "1.800005000003324\n",
      "1.3500000000000005\n",
      "1.8500000000000014\n"
     ]
    }
   ],
   "source": [
    "print(derivative(my_function,5))\n",
    "print(derivative(my_function,10))\n",
    "print(derivative(my_function,5,1e-50))\n",
    "print(derivative(my_function,10,1e-50))\n",
    "print(derivative(my_function,5,1e-4))\n",
    "print(derivative(my_function,10,1e-4))\n",
    "print(derivative(my_function,5,1))\n",
    "print(derivative(my_function,10,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Partial Derivative  \n",
    "###  (1)先寫出一個簡單的方程式：  $ f(x) = 2x^2 + 3xy + 5y^2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X=(x,y), X[0] = x, X[1]=y\n",
    "def my_function2(X):\n",
    "    # xlist is the list of the variables we have\n",
    "    # and in this particular instance we have two variables  \n",
    "    return 2*X[0]**2+3*X[0]*X[1]+5*X[1]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2)定義偏微分公式：$ \\frac{df(x_i)}{dx_a} =\\lim\\limits_{h\\to 0} \\frac{f(x_1,\\,\\cdots\\,x_a\\,+\\,h,\\,x_{a+1},\\,\\cdots\\,x_n)\\,-\\,f(x_1\\,\\cdots\\,x_n)}{h} $\n",
    ">提示：可以嘗試建立一個新的array, 然後把xlist裡面第i個位置的值update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-6\n",
    "def partial_derivative(f, X, i,epsilon=1e-6):\n",
    "    # f is our function, and i is simply the index which we are \n",
    "    # excuting our partial derivative    \n",
    "    H = X.copy()\n",
    "    h = epsilon\n",
    "    H[i] = X[i] + h\n",
    "    return (f(H) - f(X)) / h    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3)用我們的公式算 $f_x(2,\\,3)$ 和 $f_y(2,\\,3)$ 的微分\n",
    ">可以重新定義一個function來看partial對於你自己寫的function的變化<br>\n",
    ">Your answer should be around 17 and 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.000002003442205\n",
      "36.000004996594726\n"
     ]
    }
   ],
   "source": [
    "print(partial_derivative(my_function2, np.array([2., 3.]), 0,epsilon=1e-6))\n",
    "print(partial_derivative(my_function2, np.array([2., 3.]), 1,epsilon=1e-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下來讓我們自己手寫看看 $gradient$ 的程式，以下提供 $gradient$ 的算式\n",
    "### $ \\nabla f({x_1},{x_2},{x_3},\\,\\ldots) = \\frac{\\partial f}{\\partial {x_1}}i+ \\frac{\\partial f}{\\partial {x_2}}j+ \\frac{\\partial f}{\\partial {x_3}}k \\cdots$\n",
    "<hr>\n",
    "\n",
    "> 提示：可以使用 $for\\,loop$ 來寫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(f, X):\n",
    "    grad = []\n",
    "    for i in range(len(X)):\n",
    "        grad.append(partial_derivative(f, X, i))\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17.000002003442205, 36.000004996594726]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient(my_function2,np.array([2., 3.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Loss \n",
    ">Definition: a loss function or cost function is a function that maps an event or values <br> \n",
    ">of one or more variables onto a real number intuitively representing some \"cost\" <br>\n",
    ">associated with the event. <br>  \n",
    "\n",
    "常用的$loss:\\,Mean\\,Square\\,Error\\,(MSE)$ <br>\n",
    "### $MSE\\,=\\,\\frac{1}{n} \\displaystyle\\sum_{i=1}^{n} (Y_i-\\hat{Y_i})^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(true, pred):\n",
    "    MSE = 0\n",
    "    for i in range(len(true)):\n",
    "        MSE += (true[i]-pred[i])**2\n",
    "    return MSE/len(true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "Y = Y.reshape(len(Y), 1)\n",
    "names = iris.target_names\n",
    "\n",
    "# Train valid test split\n",
    "\n",
    "X_train = np.vstack([X[0:40], X[50:90], X[100:140]])\n",
    "X_valid = np.vstack([X[40:45], X[90:95], X[140:145]])\n",
    "X_test = np.vstack([X[45:50], X[95:100], X[145:150]])\n",
    "\n",
    "Y_train = np.vstack([Y[0:40], Y[50:90], Y[100:140]])\n",
    "Y_valid = np.vstack([Y[40:45], Y[90:95], Y[140:145]])\n",
    "Y_test = np.vstack([Y[45:50], Y[95:100], Y[145:150]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1)定義loss function  \n",
    "將我們所有的資料X乘上我們的Weight W (我們這邊先不管Bias)  \n",
    "然後我們使用的是MSE  \n",
    "所以我們要套用MSE的算式如以下  \n",
    "\n",
    "### $f\\,=\\,\\frac{1}{2n} \\displaystyle\\sum_{i=1}^{n} (Y_i-XW)^2$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function(W, data=X_train, target=Y_train):\n",
    "    #矩陣相乘\n",
    "    Z = np.matmul(data, W)\n",
    "    f = MSE(target, Z)/2\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gradient_descent  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss of the 1 epoch is 42.1148 , and the validation loss of the 1 epoch is 39.4722\n",
      "The training loss of the 11 epoch is 2.715 , and the validation loss of the 11 epoch is 2.4563\n",
      "The training loss of the 21 epoch is 0.2498 , and the validation loss of the 21 epoch is 0.1886\n",
      "The training loss of the 31 epoch is 0.0764 , and the validation loss of the 31 epoch is 0.0403\n",
      "The training loss of the 41 epoch is 0.0593 , and the validation loss of the 41 epoch is 0.027\n",
      "The training loss of the 51 epoch is 0.0565 , and the validation loss of the 51 epoch is 0.0244\n",
      "The training loss of the 61 epoch is 0.0557 , and the validation loss of the 61 epoch is 0.0234\n",
      "The training loss of the 71 epoch is 0.0554 , and the validation loss of the 71 epoch is 0.0229\n",
      "The training loss of the 81 epoch is 0.0553 , and the validation loss of the 81 epoch is 0.0227\n",
      "The training loss of the 91 epoch is 0.0551 , and the validation loss of the 91 epoch is 0.0225\n"
     ]
    }
   ],
   "source": [
    "def gradient_descent(X_train, Y_train, X_valid, Y_valid, W, alpha, num_iters):\n",
    "    m = len(Y_train)\n",
    "    train_loss = np.zeros((num_iters, 1))\n",
    "    valid_loss = np.zeros((num_iters, 1))\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # function代W求維分\n",
    "        delta = gradient(function, W)\n",
    "        delta = np.array(delta)\n",
    "        W -= alpha*delta\n",
    "        \n",
    "        train_loss[i] = MSE(Y_train, np.matmul(X_train, W))\n",
    "        valid_loss[i] = MSE(Y_valid, np.matmul(X_valid, W))\n",
    "        \n",
    "        if i%10 == 0:\n",
    "            print('The training loss of the', i+1, 'epoch is', train_loss[i][0].round(4), \n",
    "                  ', and the validation loss of the', i+1, 'epoch is', valid_loss[i][0].round(4))\n",
    "    return W, train_loss, valid_loss\n",
    "\n",
    "# Initializing \n",
    "np.random.seed(37)\n",
    "W = np.random.random((4,1))\n",
    "W, train_loss, valid_loss = gradient_descent(X_train, Y_train, X_valid, Y_valid, W, 0.03, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.04607155]\n",
      " [-0.17754254]\n",
      " [ 0.13631098]\n",
      " [ 0.63300481]]\n",
      "The MSE score of our prediction is  0.0\n"
     ]
    }
   ],
   "source": [
    "print(W)\n",
    "predict = np.matmul(X_test, W).round()\n",
    "print('The MSE score of our prediction is ', MSE(Y_test, predict)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP\n",
    "<hr>\n",
    "Sigmoid:  \n",
    "Sigmoid (AKA logistic regression) 是一個用來把資料壓到 0 跟 1 之間的一個 function<br>\n",
    "雖然 Sigmoid 非常好用 <br>\n",
    "不過之後的課程也會提到 Sigmoid 的一些壞處<br>\n",
    "請各位另用以下的算式寫出一個 Sigmoid 的 function<br>\n",
    "\n",
    "\n",
    "### $ Z(x_i) = \\frac{1}{1+e^{-x_i}}$\n",
    "<hr>\n",
    "\n",
    ">提示：可以使用 np.exp 來算 e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5       , 0.73105858, 0.88079708, 0.95257413, 0.98201379])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define activation: sigmoid\n",
    "def sigmoid(X):\n",
    "    output = 1 / (1 + np.exp(-X)) \n",
    "    return output\n",
    "\n",
    "# Examples\n",
    "X = np.arange(5)\n",
    "sigmoid(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Gradient: \n",
    "sigmoid gradient的式子<br>\n",
    "### $ \\frac{d}{dx} Z(x_i) = Z(x_i) \\times (1-Z(x_i))$  \n",
    "[數學推導過程](http://www.ai.mit.edu/courses/6.892/lecture8-html/sld015.htm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_gradient(X):\n",
    "    output = sigmoid(X)*(1-sigmoid(X))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax:\n",
    "<hr>\n",
    "Softmax (AKA normalized exponential function) 與 Sigmoid 有點類似 <br>\n",
    "不過 Softmax 是針對一整組數字做壓縮 <br>\n",
    "而 Sigmoid 是直接對單一的數值做壓縮\n",
    "請各位另用以下的算式寫出一個 Softmax 的 function<br>\n",
    "\n",
    "\n",
    "### $\\sigma(\\mathbf{x})_j = \\frac{e^{x_j}}{\\sum_{i=1}^K e^{x_i}}\\,for\\,j\\,\\in\\,1,\\,\\ldots\\,,\\,K$ \n",
    "\n",
    "For example:<br>\n",
    "如果我們有一組數列 $1,\\,3,\\,5$ <br>\n",
    "Softmax 會回傳 $0.016,\\,0.117,\\,0.867$\n",
    "\n",
    "<hr>\n",
    "\n",
    ">提示：python 會自動 broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01165623, 0.03168492, 0.08612854, 0.23412166, 0.63640865]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define activation: softmax\n",
    "def softmax(X): \n",
    "    return np.exp(X) / np.sum(np.exp(X), axis=1, keepdims=True)\n",
    "\n",
    "# Examples\n",
    "X = np.array([np.arange(5)])\n",
    "softmax(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross entropy (Multiclass)\n",
    "<hr>\n",
    "Definition: Cross-entropy loss, or log loss, measures the performance of a classification model<br>\n",
    "whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted<br>\n",
    "probability diverges from the actual label. So predicting a probability of .012 when the actual<br>\n",
    "observation label is 1 would be bad and result in a high loss value. A perfect model would have a <br>\n",
    "log loss of 0.<br><br>\n",
    "Cross entropy 的算式如下：<br>\n",
    "\n",
    "### $H(p,q)=-\\displaystyle\\sum _{i=1}^m  p(x_i)\\,\\log q(x_i).\\!$\n",
    "在這邊 p 代表的是我們的實際值，q 代表的是我們的預測值，$x_i$ 是我們的samples，m 是sample總數。<br>\n",
    "如果還是不太清楚的話以下提供一個例子： <br><br>\n",
    ">假設我們的第一個Y值(實際值)為 \\[1, 0, 0\\]，預測值為 \\[0.7, 0.2, 0.1\\] 的話\n",
    ">#### $p(x_{0})\\,\\log q(x_{0})=1\\times \\log 0.7 + 0\\times \\log 0.2 + 0\\times \\log 0.1 \\approx -0.357$\n",
    "<!---\n",
    "1. 假設我們的實際值為0，預測值為0.1的話\n",
    "#### $-\\sum_{x_i}p(x_{i})\\,\\log q(x_{i})=0\\times \\log 0.1 +(1-0) \\times \\log (1-0.1) = 0.105$\n",
    "--->\n",
    "這邊會是負數是因為log 1 到 0 之間的數值的話都會回傳負值<br>\n",
    "但是沒關係  <br>\n",
    "我們做完sum之後會在乘上 -1  <br><br>\n",
    "請大家用這個算式自己寫寫看一個cross entropy 的function <br>\n",
    "<hr>\n",
    "\n",
    "><b>提示：</b> <br>\n",
    ">由於log(0)會出現錯誤，所以我們通常會加一個epsilon，通常會設定為1e-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.356674943938731"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# p , q is list\n",
    "def cross_entropy(p, q):\n",
    "    epsilon = 1e-15\n",
    "    H = 0\n",
    "    for i in range(len(p)):\n",
    "        # epsilon avoid 0\n",
    "        H += -p[i]*np.log(q[i]+epsilon)       \n",
    "    H = H.sum()/p.shape[0]\n",
    "    return H\n",
    "\n",
    "p = np.array([[1,0,0]])\n",
    "q = np.array([[0.7, 0.2, 0.1]])\n",
    "cross_entropy(p,q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding\n",
    "<hr>\n",
    "One Hot Encoding 做的事情就是把一個有 N 個不同種類的 array<br>\n",
    "變成一個有 N 個 Column 的矩陣<br>\n",
    "然後每一個 Column 都只會出現 1 或是 0<br>\n",
    "每一個 Column 也都對應到 N 個種類中的其中一種<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def one_hot_encoding(array):\n",
    "    \n",
    "    sorted_array = np.sort(array)\n",
    "    count = 1\n",
    "    unique = [sorted_array[0]]\n",
    "    \n",
    "    temp = sorted_array[0]\n",
    "    for i in range(len(array)):\n",
    "        if sorted_array[i] != temp:\n",
    "            count += 1\n",
    "            temp = sorted_array[i]\n",
    "            unique.append(temp)\n",
    "            \n",
    "    eye = np.zeros((len(unique), len(unique)))\n",
    "    for i in range(len(unique)):\n",
    "        eye[i, i] = 1\n",
    "    \n",
    "    for i in range(len(array)):\n",
    "        for j in range(len(unique)):\n",
    "            if array[i] == unique[j]:\n",
    "                array[i] = j\n",
    "                break\n",
    "                \n",
    "    result = eye[array]\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example.\n",
    "array = np.array([1,2,3,2])\n",
    "one_hot_encoding(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神經網路架構<br>\n",
    "流程是<br>\n",
    "Forward\n",
    "1. $layer_1 = X \\times weight_1 $\n",
    "2. $activation_1 = activation(z_1)$\n",
    "3. $layer_2 = a_1 \\times weight_2$\n",
    "4. $prediction = activationo(z_2)$<br>\n",
    "\n",
    "\n",
    "Loss Function<br>\n",
    "\n",
    "1. $loss = loss\\,function(true\\,value,\\,prediction)$<br>\n",
    "\n",
    "Backward<br>\n",
    "\n",
    "1. $derivative\\,of\\,layer_2 = prediction - true\\,value$\n",
    "2. $derivative\\,of\\,weight_2  = activation_1\\times derivative\\,of\\,layer_2$\n",
    "3. $derivative\\,of\\,layer_1 = derivative\\,of\\,layer_2\\times weight_2\\times gradient\\,of\\,activation_1$\n",
    "4. $derivative\\,of\\,weight_1  = X\\times derivative\\,of\\,layer_1$\n",
    "\n",
    "兩件事情<br>\n",
    "第一件事情是我們這邊沒有加上bias<br>\n",
    "這邊沒有加上bias是因為會讓model變得比較複雜<br>\n",
    "為了簡化model所以沒有加上去<br>\n",
    "第二件事情是Backward的第一步<br>\n",
    "為什麼沒有做 softmax 和 cross entropy 的微分<br>\n",
    "然後就用 prediction 減掉 true value<br>\n",
    "其實是因為這個就是 cross entropy 加上 softmax 後的微分<br>\n",
    "[Beautiful Math](https://deepnotes.io/softmax-crossentropy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_layer_net(X, Y, W1, W2):\n",
    "    # Forward\n",
    "    z1 = np.matmul(X, W1) \n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.matmul(a1, W2) \n",
    "    out = softmax(z2)\n",
    "    J = cross_entropy(Y, out)\n",
    "    # Backward\n",
    "    d2 = out - Y\n",
    "    dW2 = np.matmul(a1.T, d2)\n",
    "    d1 = np.matmul(d2, (W2.T))*sigmoid_gradient(a1)\n",
    "    dW1 = np.matmul(X.T, d1)\n",
    "    \n",
    "    return J, dW1, dW2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data\n",
    ">提示：如果 Function output 三個東西可是我們只要一個東西的話<br>\n",
    ">     我們可以用底線 _ 來忽略\n",
    "```python \n",
    "J_valid, _, _ = two_layer_net......\n",
    "``` \n",
    "><br>或是我們可以直接用中括弧跟數字來代表<br>\n",
    "```python\n",
    "J_valid = two_layer_net(......)[0]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss of the 50 epoch is 0.5166 , The validation loss of the 50 epoch is 0.5436\n",
      "The training loss of the 100 epoch is 0.4769 , The validation loss of the 100 epoch is 0.5154\n",
      "The training loss of the 150 epoch is 0.4492 , The validation loss of the 150 epoch is 0.492\n",
      "The training loss of the 200 epoch is 0.3166 , The validation loss of the 200 epoch is 0.2497\n",
      "The training loss of the 250 epoch is 0.244 , The validation loss of the 250 epoch is 0.206\n",
      "The training loss of the 300 epoch is 0.1796 , The validation loss of the 300 epoch is 0.1811\n",
      "The training loss of the 350 epoch is 0.119 , The validation loss of the 350 epoch is 0.1182\n",
      "The training loss of the 400 epoch is 0.1348 , The validation loss of the 400 epoch is 0.0478\n",
      "The training loss of the 450 epoch is 0.1349 , The validation loss of the 450 epoch is 0.0451\n",
      "The training loss of the 500 epoch is 0.1336 , The validation loss of the 500 epoch is 0.0412\n",
      "The training loss of the 550 epoch is 0.1308 , The validation loss of the 550 epoch is 0.0373\n",
      "The training loss of the 600 epoch is 0.1283 , The validation loss of the 600 epoch is 0.0343\n",
      "The training loss of the 650 epoch is 0.1264 , The validation loss of the 650 epoch is 0.0322\n",
      "The training loss of the 700 epoch is 0.1249 , The validation loss of the 700 epoch is 0.0306\n",
      "The training loss of the 750 epoch is 0.1238 , The validation loss of the 750 epoch is 0.0293\n",
      "The training loss of the 800 epoch is 0.1228 , The validation loss of the 800 epoch is 0.0282\n",
      "The training loss of the 850 epoch is 0.1221 , The validation loss of the 850 epoch is 0.0272\n",
      "The training loss of the 900 epoch is 0.1215 , The validation loss of the 900 epoch is 0.0261\n",
      "The training loss of the 950 epoch is 0.1212 , The validation loss of the 950 epoch is 0.0251\n",
      "The training loss of the 1000 epoch is 0.1213 , The validation loss of the 1000 epoch is 0.0239\n",
      "\n",
      "The loss of our testing set is  0.0249\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "Y = one_hot_encoding(Y)\n",
    "names = iris.target_names\n",
    "\n",
    "X_train = np.vstack([X[0:40], X[50:90], X[100:140]])\n",
    "X_valid = np.vstack([X[40:45], X[90:95], X[140:145]])\n",
    "X_test = np.vstack([X[45:50], X[95:100], X[145:150]])\n",
    "\n",
    "Y_train = np.vstack([Y[0:40], Y[50:90], Y[100:140]])\n",
    "Y_valid = np.vstack([Y[40:45], Y[90:95], Y[140:145]])\n",
    "Y_test = np.vstack([Y[45:50], Y[95:100], Y[145:150]])\n",
    "#----------------------------------------------------\n",
    "iteration = 1000\n",
    "alpha = 0.01\n",
    "history_train = np.zeros((iteration, 1))\n",
    "history_valid = np.zeros((iteration, 1))\n",
    "\n",
    "np.random.seed(37)\n",
    "W1 = np.random.randn(4,10)\n",
    "W2 = np.random.randn(10,3)\n",
    "\n",
    "for i in range(iteration):\n",
    "    J_train, dW1, dW2 = two_layer_net(X_train, Y_train, W1, W2)\n",
    "    J_valid, _, _ = two_layer_net(X_valid, Y_valid, W1, W2)\n",
    "    W1 -= alpha*dW1\n",
    "    #b1 -= alpha*db1\n",
    "    W2 -= alpha*dW2\n",
    "    #b2 -= alpha*db2\n",
    "    \n",
    "    history_train[i] = J_train\n",
    "    history_valid[i] = J_valid\n",
    "    if (i+1)%50 == 0:\n",
    "        print('The training loss of the', i+1, 'epoch is', history_train[i][0].round(4), ', ', end='')\n",
    "        print('The validation loss of the', i+1, 'epoch is', history_valid[i][0].round(4))\n",
    "        \n",
    "print('\\nThe loss of our testing set is ', two_layer_net(X_test, Y_test, W1, W2)[0].round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myKernel",
   "language": "python",
   "name": "mykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
