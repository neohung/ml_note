{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP\n",
    "\n",
    "Derivative 微分:  \n",
    "定義微分公式：$ \\frac{df(x)}{dx} =\\lim\\limits_{h\\to 0} \\frac{(f(x+h) - f(x))}{h}$  \n",
    "定義偏微分公式：$ \\frac{df(x_i)}{dx_a} =\\lim\\limits_{h\\to 0} \\frac{f(x_1,\\,\\cdots\\,x_a\\,+\\,h,\\,x_{a+1},\\,\\cdots\\,x_n)\\,-\\,f(x_1\\,\\cdots\\,x_n)}{h} $  \n",
    "#### Gradient:  \n",
    "$\\nabla f({x_1},{x_2},{x_3},\\,\\ldots) = \\frac{\\partial f}{\\partial {x_1}}i+ \\frac{\\partial f}{\\partial {x_2}}j+ \\frac{\\partial f}{\\partial {x_3}}k \\cdots$  \n",
    "#### Loss:  \n",
    "\n",
    "常用的$loss:\\,Mean\\,Square\\,Error\\,(MSE)$ <br>\n",
    "$MSE\\,=\\,\\frac{1}{n} \\displaystyle\\sum_{i=1}^{n} (Y_i-\\hat{Y_i})^2$  \n",
    "對於$\\hat y = wx$(不考慮bias), 設計loss function如下:  \n",
    "$f\\,=\\,\\frac{1}{2n} \\displaystyle\\sum_{i=1}^{n} (Y_i-XW)^2$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(123)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss of the 1 epoch is 42.1148 , and the validation loss of the 1 epoch is 39.4722\n",
      "The training loss of the 11 epoch is 2.715 , and the validation loss of the 11 epoch is 2.4563\n",
      "The training loss of the 21 epoch is 0.2498 , and the validation loss of the 21 epoch is 0.1886\n",
      "The training loss of the 31 epoch is 0.0764 , and the validation loss of the 31 epoch is 0.0403\n",
      "The training loss of the 41 epoch is 0.0593 , and the validation loss of the 41 epoch is 0.027\n",
      "The training loss of the 51 epoch is 0.0565 , and the validation loss of the 51 epoch is 0.0244\n",
      "The training loss of the 61 epoch is 0.0557 , and the validation loss of the 61 epoch is 0.0234\n",
      "The training loss of the 71 epoch is 0.0554 , and the validation loss of the 71 epoch is 0.0229\n",
      "The training loss of the 81 epoch is 0.0553 , and the validation loss of the 81 epoch is 0.0227\n",
      "The training loss of the 91 epoch is 0.0551 , and the validation loss of the 91 epoch is 0.0225\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "Y = Y.reshape(len(Y), 1)\n",
    "names = iris.target_names\n",
    "\n",
    "# Train valid test split\n",
    "\n",
    "X_train = np.vstack([X[0:40], X[50:90], X[100:140]])\n",
    "X_valid = np.vstack([X[40:45], X[90:95], X[140:145]])\n",
    "X_test = np.vstack([X[45:50], X[95:100], X[145:150]])\n",
    "\n",
    "Y_train = np.vstack([Y[0:40], Y[50:90], Y[100:140]])\n",
    "Y_valid = np.vstack([Y[40:45], Y[90:95], Y[140:145]])\n",
    "Y_test = np.vstack([Y[45:50], Y[95:100], Y[145:150]])\n",
    "\n",
    "def derivative(f, x, epsilon=0.1):\n",
    "    # f is our function, and x is our variable\n",
    "    h = epsilon\n",
    "    return (f(x+h) - f(x)) / h\n",
    "\n",
    "def partial_derivative(f, X, i,epsilon=1e-6):\n",
    "    # f is our function, and i is simply the index which we are \n",
    "    # excuting our partial derivative    \n",
    "    H = X.copy()\n",
    "    h = epsilon\n",
    "    H[i] = X[i] + h\n",
    "    return (f(H) - f(X)) / h    \n",
    "\n",
    "def gradient(f, X):\n",
    "    grad = []\n",
    "    for i in range(len(X)):\n",
    "        grad.append(partial_derivative(f, X, i))\n",
    "    return grad\n",
    "\n",
    "def MSE(true, pred):\n",
    "    MSE = 0\n",
    "    for i in range(len(true)):\n",
    "        MSE += (true[i]-pred[i])**2\n",
    "    return MSE/len(true)\n",
    "\n",
    "def loss_function(W, data=X_train, target=Y_train):\n",
    "    #矩陣相乘\n",
    "    Z = np.dot(data, W)\n",
    "    f = MSE(target, Z)/2\n",
    "    return f\n",
    "\n",
    "def gradient_descent(X_train, Y_train, X_valid, Y_valid, W, alpha, num_iters):\n",
    "    m = len(Y_train)\n",
    "    train_loss = np.zeros((num_iters, 1))\n",
    "    valid_loss = np.zeros((num_iters, 1))\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # function代W求維分\n",
    "        delta = gradient(loss_function, W)\n",
    "        delta = np.array(delta)\n",
    "        W -= alpha*delta\n",
    "        \n",
    "        train_loss[i] = MSE(Y_train, np.dot(X_train, W))\n",
    "        valid_loss[i] = MSE(Y_valid, np.dot(X_valid, W))\n",
    "        \n",
    "        if i%10 == 0:\n",
    "            print('The training loss of the', i+1, 'epoch is', train_loss[i][0].round(4), \n",
    "                  ', and the validation loss of the', i+1, 'epoch is', valid_loss[i][0].round(4))\n",
    "    return W, train_loss, valid_loss\n",
    "\n",
    "np.random.seed(37)\n",
    "W = np.random.random((4,1))\n",
    "W, train_loss, valid_loss = gradient_descent(X_train, Y_train, X_valid, Y_valid, W, 0.03, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE score of our prediction is  0.0\n"
     ]
    }
   ],
   "source": [
    "#輸出預測結果\n",
    "predict = np.dot(X_test, W).round()\n",
    "print('The MSE score of our prediction is ', MSE(Y_test, predict)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid (AKA logistic regression)\n",
    "用來把資料壓縮到0跟1之間的函數  \n",
    "$ Z(x_i) = \\frac{1}{1+e^{-x_i}}$  \n",
    "Sigmoid Gradient:  \n",
    "$ \\frac{d}{dx} Z(x_i) = Z(x_i) \\times (1-Z(x_i))$  \n",
    "\n",
    "### Softmax (AKA normalized exponential function):  \n",
    "與 Sigmoid 有點類似 <br>\n",
    "不過 Softmax 是針對一整組數字做壓縮 <br>\n",
    "而 Sigmoid 是直接對單一的數值做壓縮  \n",
    "$\\sigma(\\mathbf{x})_j = \\frac{e^{x_j}}{\\sum_{i=1}^K e^{x_i}}\\,for\\,j\\,\\in\\,1,\\,\\ldots\\,,\\,K$  \n",
    "\n",
    "For example:<br>\n",
    "如果我們有一組數列 $1,\\,3,\\,5$ <br>\n",
    "Softmax 會回傳 $0.016,\\,0.117,\\,0.867$\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    output = 1 / (1 + np.exp(-X)) \n",
    "    return output\n",
    "def sigmoid_gradient(X):\n",
    "    output = sigmoid(X)*(1-sigmoid(X))\n",
    "    return output\n",
    "def softmax(X): \n",
    "    #np.sum()可以使用keepdims讓維度(R,)變(R,1)\n",
    "    return np.exp(X) / np.sum(np.exp(X), axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross entropy (Multiclass下)  \n",
    "$H(p,q)=-\\displaystyle\\sum _{i=1}^m  p(x_i)\\,\\log q(x_i).\\!$  \n",
    ">假設我們的實際值為0，預測值為0.1的話  \n",
    ">$-\\sum_{x_i}p(x_{i})\\,\\log q(x_{i})=0\\times \\log 0.1 +(1-0) \\times \\log (1-0.1) = 0.105$  \n",
    ">假設我們的第一個Y值(實際值)為 \\[1, 0, 0\\]，預測值為 \\[0.7, 0.2, 0.1\\] 的話  \n",
    ">$p(x_{0})\\,\\log q(x_{0})=1\\times \\log 0.7 + 0\\times \\log 0.2 + 0\\times \\log 0.1 \\approx -0.357$\n",
    "\n",
    "加epsilon避免log(0)發生  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.356674943938731"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# p , q is list\n",
    "def cross_entropy(p, q):\n",
    "    epsilon = 1e-15\n",
    "    H = 0\n",
    "    for i in range(len(p)):\n",
    "        # epsilon avoid 0\n",
    "        H += -p[i]*np.log(q[i]+epsilon)       \n",
    "    H = H.sum()/p.shape[0]\n",
    "    return H\n",
    "\n",
    "p = np.array([[1,0,0]])\n",
    "q = np.array([[0.7, 0.2, 0.1]])\n",
    "cross_entropy(p,q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def one_hot_encoding(array):\n",
    "    \n",
    "    sorted_array = np.sort(array)\n",
    "    count = 1\n",
    "    #unique會將sorted_array重複部分去掉\n",
    "    unique = [sorted_array[0]]\n",
    "    \n",
    "    temp = sorted_array[0]\n",
    "    for i in range(len(array)):\n",
    "        if sorted_array[i] != temp:\n",
    "            count += 1\n",
    "            temp = sorted_array[i]\n",
    "            unique.append(temp)\n",
    "    \n",
    "    eye = np.zeros((len(unique), len(unique)))\n",
    "    for i in range(len(unique)):\n",
    "        eye[i, i] = 1\n",
    "    #做出eye矩陣\n",
    "    for i in range(len(array)):\n",
    "        for j in range(len(unique)):\n",
    "            if array[i] == unique[j]:\n",
    "                #將array中的元素轉成eye矩陣的index索引\n",
    "                array[i] = j\n",
    "                break\n",
    "    result = eye[array]\n",
    "    return result\n",
    "\n",
    "# Example.\n",
    "one_hot_encoding(np.array([1,2,3,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神經網路架構  \n",
    "\n",
    "z = $w_1x_1+w_2x_2...+w_nx_n+b$  \n",
    "a = $\\hat y=\\sigma(z)$\n",
    "\n",
    "$w_{ij}^l$  指的是第l-1到l層中l層第i個neuron相對l-1層第j個neuron的邊  \n",
    "$z_{i}^l$  指的是第l層中第i個neuron的z  \n",
    "$a_{i}^l$  指的是第l層中第i個neuron的a  \n",
    "$x_{j}$  指的是輸入層中第j個feature  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "流程是  \n",
    "Forward  \n",
    "1. $layer_1 = X \\times weight_1 $, 這裏X維度120x4, W1維度4X10, layer1有120x10維  \n",
    "2. $activation_1 = activation(z_1)$, 這裏用sigmoid函數  \n",
    "3. $layer_2 = a_1 \\times weight_2$, 這裏a1維度120x10, W1維度10X3, 因為y被one hot encode成3維的標籤  \n",
    "4. $prediction = activationo(z_2)$, 這裏用softmax壓縮到0跟1之間  \n",
    "<br>   \n",
    "\n",
    "Backward  \n",
    "假設資料有R筆$(x^1,y^1),..,(x^r,x^r)..,(x^R,x^R)$  \n",
    "令loss: $L(w) = \\frac{1}{R}||\\sum(f(x^r;w)-y^r)||$  \n",
    "要更新$w_{ij}^l$需要求$\\frac{\\partial L^r}{\\partial w_{ij}^l} :$  \n",
    "$\\frac{\\partial L^r}{\\partial w_{ij}^l}=\\frac{\\partial z_{ij}^l}{\\partial w_{ij}^l}*\\frac{\\partial L^r}{\\partial z_{ij}^l}$  \n",
    "第1項$\\frac{\\partial z_{ij}^l}{\\partial w_{ij}^l}$:  \n",
    "> 對於輸入層:  \n",
    "$z_{ij}^1 = w_{ij}^1x_j^r + b_i^1$,  $\\frac{\\partial z_{ij}^l}{\\partial w_{ij}^l}=x_j^r$  \n",
    "> 對於l-1到l層:  \n",
    "$z_{ij}^l = w_{ij}^la_j^{l-1} + b_i^l$,  $\\frac{\\partial z_{ij}^l}{\\partial w_{ij}^l}=a_j^{l-1}$ \n",
    "\n",
    "第2項$\\frac{\\partial L^r}{\\partial z_{ij}^l}$: \n",
    "> 對於輸出層(第L層):  \n",
    "> $\\frac{\\partial L^r}{\\partial z_{ij}^L}= \\frac{\\partial L^r}{\\partial y}\\frac{\\partial y}{\\partial z{ij}^L}$, 以$loss=\\frac{1}{2}(\\hat y - y)^2, \\frac{\\partial L^r}{\\partial y}=(\\hat y - y)$, $\\frac{\\partial y}{\\partial z{ij}^L}=\\sigma'(z_{ij}^L)$, 這裏如果激發函數是sigmoid的話$\\sigma'(z_{ij}^L)=\\sigma(z_{ij}^L)(1-\\sigma(z_{ij}^L))$   \n",
    "> 以$loss=cross\\,entropy(softmax(\\hat y)), $微分=$(\\hat y - y)$  \n",
    "> 對於l-1到l層:  \n",
    "> 令$\\frac{\\partial L^r}{\\partial z_{ij}^l}=\\delta_i^l = \\sigma'(z_i^l)\\sum_kw_{ki}^{l+1}\\delta_k^{l+1}$\n",
    "\n",
    "$\\frac{\\partial L^r}{\\partial b_i^l}$, 也是一樣的方式計算  \n",
    "\n",
    "\n",
    "1. $derivative\\,of\\,layer_2 = prediction - true\\,value$, 算出預測值與實際值誤差,d2維度120x3  \n",
    "1. 剛好cross entropy 加上 softmax 後的微分, 就是prediction - true\\,value  \n",
    "1. $derivative\\,of\\,weight_2  = activation_1\\times derivative\\,of\\,layer_2$, a1維度120x10, d2維度120x3 dw2=a1.Txd2維度10x3   \n",
    "1. $derivative\\,of\\,layer_1 = derivative\\,of\\,layer_2\\times weight_2\\times gradient\\,of\\,activation_1$, \n",
    "1. $derivative\\,of\\,weight_1  = X\\times derivative\\,of\\,layer_1$  \n",
    "\n",
    "\n",
    "### 假設只有兩層的網路:  \n",
    "Forward: 先求出w,z,a,$\\hat y$  \n",
    "1. $z1=w_1^1x_1+w_2^1x_2+..= X*w1$    \n",
    "1. $a1=\\sigma(z1)=sigmoid(z1)$  \n",
    "1. $z2=w_1^2a_1+w_2^2a_2+..= a1*w2$  \n",
    "1. $\\hat y=\\sigma(z2)=softmax(z2)$  \n",
    "1. 定義Loss Function:$L^r=J(\\theta)=$Cross Entropy($\\hat y,y$)  \n",
    "\n",
    "Backward: 求出$\\frac{\\partial L^r}{\\partial w_{ij}^l}$  \n",
    "1. 先求$\\delta^2$: 對於輸出層softmax加上Loss使用cross entropy, d2=$\\delta_i^2$=$\\frac{\\partial L^r}{\\partial z_{i}^2}=(\\hat y_i - y_i)$   \n",
    "1. $w^2$的微分$\\frac{\\partial L^r}{\\partial w_{ij}^2}=\\frac{\\partial L^r}{\\partial z_{i}^2}\\frac{\\partial z_i^2}{\\partial w_{ij}^2}= (\\hat y_i - y_i)a_i^1$, 所以 $dw2=a1^T * d2$   \n",
    "1. 接著求$\\delta^1$: d1=$\\frac{\\partial L^r}{\\partial z_{i}^1}=\\delta_i^1 = \\sigma'(z_i^1)\\sum_kw_{ki}^2\\delta_i^2$\n",
    "$=\\sigma'(z_i^1) ( d2*{w^2}^T )$\n",
    "1. $w^1$的微分$\\frac{\\partial L^r}{\\partial w_{ij}^1}=\\frac{\\partial L^r}{\\partial z_{i}^1}\\frac{\\partial z_i^1}{\\partial w_{ij}^1}$, 所以dw1$=X^T*d1$\n",
    "\n",
    "這邊簡化問題沒有加上bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4)\n",
      "(120, 10)\n",
      "(120, 10)\n",
      "(120, 3)\n",
      "(120, 10)\n",
      "(120, 10)\n"
     ]
    }
   ],
   "source": [
    "def two_layer_net(X, Y, W1, W2):\n",
    "    #X[0]=第一筆訓練樣本, W1\n",
    "    #Forward\n",
    "    # z1每筆輸入4維資料輸出向量10維的資料, 共120筆, 所以z1維度120x10\n",
    "    z1 = np.dot(X,W1)\n",
    "    #將每筆資料壓縮本身到0,1之間\n",
    "    a1 = sigmoid(z1)\n",
    "    # z2每筆輸入10維資料輸出向量3維的資料, 共120筆, 所以z2維度120x3\n",
    "    z2 = np.dot(a1, W2)\n",
    "    #將所有資料壓縮到0,1之間\n",
    "    out = softmax(z2)\n",
    "    \n",
    "    \n",
    "\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "#Y被encode成3維\n",
    "Y = one_hot_encoding(Y)\n",
    "names = iris.target_names\n",
    "\n",
    "X_train = np.vstack([X[0:40], X[50:90], X[100:140]])\n",
    "X_valid = np.vstack([X[40:45], X[90:95], X[140:145]])\n",
    "X_test = np.vstack([X[45:50], X[95:100], X[145:150]])\n",
    "\n",
    "Y_train = np.vstack([Y[0:40], Y[50:90], Y[100:140]])\n",
    "Y_valid = np.vstack([Y[40:45], Y[90:95], Y[140:145]])\n",
    "Y_test = np.vstack([Y[45:50], Y[95:100], Y[145:150]])\n",
    "#----------------------------------------------------\n",
    "iteration = 1000\n",
    "alpha = 0.01\n",
    "history_train = np.zeros((iteration, 1))\n",
    "history_valid = np.zeros((iteration, 1))\n",
    "\n",
    "np.random.seed(37)\n",
    "#X有4維, W1有4x10, 表示L1有10個node\n",
    "W1 = np.random.randn(4,10)\n",
    "#表示L2有3個node\n",
    "W2 = np.random.randn(10,3)\n",
    "print(X_train.shape)\n",
    "# nx4 dot 4x10 = nx10\n",
    "z = np.dot(X_train,W1)\n",
    "print(z.shape)\n",
    "# z1=120x10\n",
    "z1 = np.dot(X_train,W1)\n",
    "a1 = sigmoid(z1)\n",
    "# z2 = 120*10 dot 10x3\n",
    "z2 = np.dot(a1, W2) \n",
    "print(a1.shape)\n",
    "z2 = np.dot(a1, W2)\n",
    "print(z2.shape)\n",
    "out = softmax(z2)\n",
    "d2 = out - Y_train\n",
    "print(sigmoid_gradient(a1).shape)\n",
    "d1 = np.matmul(d2, (W2.T))*sigmoid_gradient(a1)\n",
    "print(d1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid_gradient(a1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  4  3]\n",
      " [ 8  5 12]]\n"
     ]
    }
   ],
   "source": [
    "a=np.array([[1,2,3],[4,5,6]])\n",
    "b=np.array([[1,2,1],[2,1,2]])\n",
    "print(a*b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myKernel",
   "language": "python",
   "name": "mykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
